{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Spark SQL & Machine learning\n",
    "   \n",
    "   - Beginning 2.0, DataFrame is the new basic structure in spark(built on top of RDD)\n",
    "   - A dataframe is nothing but a table with columns, rows and headers\n",
    "   - In this notebook, we will work through a dataset to demostrate spark's SQL-like abilities. We will also look at its machine learning capabilities in context of this dataset.\n",
    "    \n",
    "### Dataset\n",
    "- The chosen dataset is that of Breast Cancer. This set is collected from digitized image of a fine needle aspirate (FNA) of a breast mass. For more information about this dataset, please visit https://www.kaggle.com/uciml/breast-cancer-wisconsin-data\n",
    "- Each row contains information about a single breast mass along with a diagnosis of malignant/benign for this mass\n",
    "    \n",
    "### Problem statement\n",
    "- We need to learn patterns describing malignant and benign masses and need to be able to place any future samples in either of these buckets.  \n",
    "- Thus, the problem is a classification one where given a set of probable results, we need to pick one result with confidance. \n",
    "    \n",
    "### Solution\n",
    "   We will go through the following steps:\n",
    "   - Read data (Spark SQL)\n",
    "   - Feature Engineering (Accumulators, Broadcasters, ml/mllib APIs)\n",
    "   - Data Visualizations (PixieDust, Seaborn)\n",
    "   - Modeling (Spark ML)\n",
    "   - Evaluation and prediction (Spark ML)\n",
    "   - Deployment (Watson ML repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/martinkearn/Content/master/Blogs/Images/MLProcess.PNG\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Machine Learning process ###\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://raw.githubusercontent.com/martinkearn/Content/master/Blogs/Images/MLProcess.PNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries needed \n",
    "\n",
    "#### Copy and run the code below\n",
    "\n",
    "    from pyspark.sql import functions as F\n",
    "    from pyspark.sql.types import FloatType, StringType, IntegerType, DoubleType, ArrayType\n",
    "\n",
    "    from pyspark.mllib.stat import Statistics\n",
    "    from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "    from pyspark import AccumulatorParam\n",
    "    from pyspark import Broadcast\n",
    "\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import json\n",
    "    import pandas as pd\n",
    "\n",
    "    %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from objectStore (can be any source, really)\n",
    " Data can be read in any format (as applicable to the native format), most common use cases are reading as json,csv, text or parquet files \n",
    " \n",
    " \n",
    " Click on the data file and ask to \"Insert as SparkSession Table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename the input variable\n",
    "    data = df_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize sample dataset\n",
    "    data.select(data.columns[:9]).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register dataframe as a temp table to query from (write sql on dataframes)\n",
    "\n",
    "    data.registerTempTable(\"cancer_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL \n",
    "- NOTE that the returned object is another Dataframe\n",
    "- One nice feature of the notebooks and python is that we can show it in a table via Pandas\n",
    "\n",
    "\n",
    "##### copy and paste code \n",
    "    temp_df =  spark.sql(\"select * from cancer_data where diagnosis=='B' limit 2\")\n",
    "\n",
    "    print ('Type of result:',type(temp_df))\n",
    "    print (\"\\n\")\n",
    "    temp_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple groupby example\n",
    "   - Remember to perform an action to get your results (sql queries are also transformations :))\n",
    "    \n",
    "##### copy and paste code from below\n",
    "\n",
    "    query = \"\"\"\n",
    "    select\n",
    "        diagnosis ,\n",
    "        count(*) as diagnosis_count,\n",
    "        mean(RADIUS_SE)\n",
    "    from cancer_data\n",
    "    group by diagnosis \n",
    "    order by count(*) desc\n",
    "    \"\"\"\n",
    "\n",
    "    spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "   - Dataset = Features + target\n",
    "   - Transforming features (categorical to numeric, continous to bins, scaling, normalization etc...)\n",
    "   - Selecting a subset of columns for wide datasets\n",
    "   - Exploding columns to make additional (synthetic features) for small datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature set is numeric\n",
    "\n",
    "##### Step 1: Make sure your target is numeric\n",
    "- \"target\" is categorical (malignant/benign)\n",
    "- Our encoding: 1 = malignant(M), 0 = Benign(B) using sql like \"when\" statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### review your data\n",
    "\n",
    "    data.toPandas().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make label numeric\n",
    "    data = data.withColumnRenamed('diagnosis','label')\n",
    "    data = data.withColumn('label',F.when(data.label=='M',1).otherwise(0))\n",
    "    data.toPandas().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********************************************************************************\n",
    "\n",
    "###   Detour  \n",
    "- Feature engineering is a major task and often involves complicated scripts\n",
    "\n",
    "What happens when\n",
    "   - Have custom script based on other datasets/static variables?\n",
    "   - I want to port over a current script to spark ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulators, Broadcasters\n",
    "- One of the most basic things in scripts are variables (data and metadata). \n",
    "- Data vars = DataFrames/RDDs in spark\n",
    "- Metadata vars = Accumulators/broadcasters \n",
    "   \n",
    "- Metadata vars are not straightforward in the distributed world\n",
    "\n",
    "\n",
    "##### Accumulators\n",
    "   - Global variables which can be written into\n",
    "   \n",
    "##### Broadcasters\n",
    "   - Global variables to be read from \n",
    "************************************************************************************************\n",
    "\n",
    "\n",
    "#### Step 2: Select a subset of columns\n",
    "- Remove columns conveying very little information\n",
    "   - Zero variance columns\n",
    "   - Low correlation columns\n",
    "\n",
    "##### Zero variance test\n",
    "   - Let us write this test ourselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DictAccumulatorParam(AccumulatorParam):\n",
    "    def zero(self,initialValue):\n",
    "        return initialValue\n",
    "\n",
    "    def addInPlace(self, v1, v2):\n",
    "        v1.update(v2)\n",
    "        return v1\n",
    "    \n",
    "    \n",
    "def test_zero_variance(d,acc):\n",
    "    global df_pd\n",
    "    col = d[0]\n",
    "    value = d[1]\n",
    "    try:\n",
    "        mean = df_pd.value['mean'][col]\n",
    "        std = df_pd.value['stddev'][col]\n",
    "        if float(value)==float(mean) and float(std)==0.0:\n",
    "            acc.add({col:True})\n",
    "            return True\n",
    "        else:\n",
    "            acc.add({col:False})\n",
    "            return False\n",
    "    except KeyError:\n",
    "        acc.add({col:False})\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def count_labels(row):\n",
    "    global benign, malignant\n",
    "    if row.label==1:\n",
    "        malignant.add(1)\n",
    "    else:\n",
    "        benign.add(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcast the data variable so that each executor has its own version of it\n",
    "   - This way, it does need not be shipped to the executor with every call\n",
    "   - Saves network bandwidth\n",
    "\n",
    "##### copy and paste code from below\n",
    "    ### Create a broadcast variable with the summary stats ###\n",
    "    data = data.withColumn('dummy',F.lit(100))\n",
    "\n",
    "    ### Correct data types of variables prior to doing statistics ###\n",
    "    for col in data.columns:\n",
    "        if col not in ['ID','diagnosis']:\n",
    "            data = data.withColumn(col,data[col].cast(FloatType()))\n",
    "\n",
    "    df = data.describe()\n",
    "    df_pd = df.toPandas()\n",
    "    df_pd.index = df_pd[\"summary\"]\n",
    "    del df_pd[\"summary\"]\n",
    "    df_pd = df_pd.transpose()\n",
    "\n",
    "    df_pd = sc.broadcast(df_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and accumulate\n",
    "\n",
    "##### copy and paste code\n",
    "\n",
    "    acc = sc.accumulator({}, DictAccumulatorParam())  \n",
    "    values = data.first().asDict()\n",
    "    input_value = sc.parallelize(values.items())\n",
    "    dummy = input_value.map(lambda x:test_zero_variance(x,acc))\\\n",
    "                      .map(lambda x:(x,1))\\\n",
    "                      .reduceByKey(lambda x,y:x+y)\n",
    "    dummy_collect = dummy.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### view the contents of accumulator\n",
    "    acc.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proceed to read the accumulator to make decision to retain/drop columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing zero variance columns ###\n",
    "\n",
    "##### copy and run code from below\n",
    "    for k,v in acc.value.items():\n",
    "        if v==True:\n",
    "            data = data.drop(k)\n",
    "\n",
    "    df = data.toPandas()\n",
    "    col_corr = sorted(df.corr()['label'].to_dict().items(),key=lambda x:x[1],reverse=True)\n",
    "    for col in col_corr[-5:]:\n",
    "        if col[0]!='id':\n",
    "            data = data.drop(col[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove columns with low correlation, such columns contribute to very little information\n",
    "- Caution: Be wary of multicolinearity (\"leaky\" columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets do some graphs!\n",
    "\n",
    "###  Here, we look at the columns and their correlations with the target variable.\n",
    "- We use PixieDust (charting library) for quick charts\n",
    "- Also takes RDDs as inputs, most of the other charting librabries take pandas dataframes as inputs\n",
    "\n",
    "\n",
    "##### copy and run code from here\n",
    "    import pixiedust\n",
    "    \n",
    "    df = pd.DataFrame(col_corr)\n",
    "    df = df.dropna()\n",
    "    df.columns = ['name','correlation']\n",
    "    viz_spark_df = sqlContext.createDataFrame(df)\n",
    "\n",
    "   \n",
    "    display(viz_spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize correlations ####\n",
    " - Useful for datasets with small number of columns\n",
    " - A heatmap is useful to visualize how variables are related to each other (and not just the target)\n",
    " - Very useful while doing NLP applications to visualize similarity between documents (after using TFIDF)\n",
    " \n",
    " \n",
    "##### copy and run code from below\n",
    "    plt.style.use('ggplot')\n",
    "    values = data.rdd.map(lambda x:list(x.asDict().values()))\n",
    "    corr_values = Statistics.corr(values)\n",
    "    names = data.rdd.map(lambda x:list(x.asDict().keys())).first()\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    sns.heatmap(corr_values,xticklabels=names,yticklabels=names,square=True,vmin=0, vmax=1,\n",
    "                    cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Module layout\n",
    "##### It divides into two packages:\n",
    "    - spark.mllib contains the original API built on top of RDDs.\n",
    "    - spark.ml provides higher-level API built on top of DataFrames for constructing ML pipelines.\n",
    "    - Using spark.ml is recommended because with DataFrames the API is more versatile and flexible. But we will keep supporting spark.mllib along with the development of spark.ml. Users should be comfortable using spark.mllib features and expect more features coming.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA  - Dimensionality Reduction \n",
    " - Very handy for visualization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###### PCA Visualize data ######\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "\n",
    "import regex\n",
    "\n",
    "def g(point,co_ord):\n",
    "    if co_ord=='x':\n",
    "        value = regex.split('\\s+',point)[0][1:]\n",
    "    else:\n",
    "        \n",
    "        value =  regex.split('\\s+',point)[1][:-1]\n",
    "    return value\n",
    "\n",
    "feature_cols = list(filter(lambda x:x not in ['id','label','dummy'],data.columns))\n",
    "assembler = VectorAssembler(inputCols=feature_cols,outputCol='features')\n",
    "df = assembler.transform(data)\n",
    "\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(df)\n",
    "\n",
    "pca_result = model.transform(df)\n",
    "result = pca_result.select(\"pcaFeatures\")\n",
    "\n",
    "get_x_coord = UserDefinedFunction(lambda point:g(point,'x'), StringType())\n",
    "get_y_coord = UserDefinedFunction(lambda point:g(point,'y'), StringType())\n",
    "convert_to_string = UserDefinedFunction(lambda point:str(point.toArray()),StringType())\n",
    "\n",
    "pca_result = pca_result.withColumn('pcaFeatures_string',convert_to_string(pca_result['pcaFeatures']))\n",
    "pca_result = pca_result.withColumn('x_coord',get_x_coord(pca_result['pcaFeatures_string']))\\\n",
    "                       .withColumn('y_coord',get_y_coord(pca_result['pcaFeatures_string']))\\\n",
    "                      \n",
    "pca_result = pca_result.withColumn('x_coord',pca_result.x_coord.cast(FloatType()))\\\n",
    "                       .withColumn('y_coord',pca_result.y_coord.cast(FloatType()))\n",
    "    \n",
    "viz_df = pca_result.select('x_coord','y_coord','label').toPandas()\n",
    "a = sns.lmplot(x='x_coord',y='y_coord',hue='label',data=viz_df,\n",
    "           fit_reg=False,palette={1:'red',0:'green'},size=6,aspect=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset for machine learning\n",
    "    - Split data between test and train (70/30 split)\n",
    "    - Apply logistic regression and decision tree classifiers\n",
    "    \n",
    "##### copy and run code\n",
    "    data = data.withColumn('label',data['label'].cast(DoubleType()))\n",
    "    train, test = data.randomSplit([0.7,0.3])\n",
    "    train.count(), test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build classification models to learn your data \n",
    "- Classification models work towards making a Yes/No decision\n",
    "- Metrics to evaluate the model: Precision/Recall, AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Classification metrics ####\n",
    "def calc_metrics(results):\n",
    "    metrics = {}\n",
    "    metrics['tp_0'] = results.filter((results.label==0)&(results.prediction==0)).count()\n",
    "    metrics['fn_0'] = results.filter((results.label==0)&(results.prediction==1)).count()\n",
    "    metrics['tn_0'] = results.filter((results.label==1)&(results.prediction==1)).count()\n",
    "    metrics['fp_0'] = results.filter((results.label==1)&(results.prediction==0)).count()\n",
    "    \n",
    "    metrics['tp_1'] = results.filter((results.label==1)&(results.prediction==1)).count()\n",
    "    metrics['fn_1'] = results.filter((results.label==1)&(results.prediction==0)).count()\n",
    "    metrics['tn_1'] = results.filter((results.label==0)&(results.prediction==0)).count()\n",
    "    metrics['fp_1'] = results.filter((results.label==0)&(results.prediction==1)).count()\n",
    "    \n",
    "    return metrics\n",
    "        \n",
    "### calc precision & recall ###\n",
    "def precision_recall(results):\n",
    "    items = calc_metrics(results)\n",
    "    pre_0 = items['tp_0']/float((items['tp_0']+items['fp_0']))\n",
    "    pre_1 = items['tp_1']/float((items['tp_1']+items['fp_1']))\n",
    "    \n",
    "    recall_0 = items['tp_0']/float((items['tp_0']+items['fn_0']))\n",
    "    recall_1 = items['tp_1']/float((items['tp_1']+items['fn_1']))\n",
    "    \n",
    "    return {'pre_0':pre_0, 'recall_0':recall_0,'pre_1':pre_1,'recall_1':recall_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "- Easy to use API\n",
    "- Define stages to transform your dataset\n",
    "- Select any estimator (model) for prediction \n",
    "- Each pipeline will model a single estimator onto your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Assembler to combine feature into a single vetor (feature vector! :) )\n",
    "\n",
    "#### copy and run code\n",
    "    from pyspark.ml.linalg import Vectors #linear algebra package, has matrices, arrays, Vectors (dense and sparse)\n",
    "    from pyspark.ml.feature import VectorAssembler \n",
    "\n",
    "    feature_cols = list(filter(lambda x:x not in ['id','label'],data.columns))\n",
    "    assembler = VectorAssembler(inputCols=feature_cols,outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Pipelines \n",
    "- Logistic Regression\n",
    "- Tree based classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic regression #####\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "    #define estimator and fit data\n",
    "    estimator = LogisticRegression()\n",
    "    pipeline = Pipeline(stages=[assembler,estimator])\n",
    "    lr_model = pipeline.fit(train)\n",
    "\n",
    "    #get results\n",
    "    results = lr_model.transform(test)\n",
    "    precision_recall(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Visualize results from logistic regression ###\n",
    "pca_subset = pca_result.select('id','features','x_coord','y_coord')\n",
    "viz_df = pca_subset.join(results,on=['id','features']).select('x_coord','y_coord','prediction','label')\n",
    "viz_df = viz_df.dropna(how='any',subset=['x_coord','y_coord']).toPandas()\n",
    "\n",
    "a = sns.lmplot(x='x_coord',y='y_coord',hue='prediction',col='label',data=viz_df,\n",
    "           fit_reg=False,palette={1:'red',0:'green'},size=6,aspect=1)\n",
    "a.set_titles(col_template=['Decision Tree results when label==\"Benign\"','Decision Tree results when label==\"Malignant\"'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree classifier ####\n",
    "    from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "    # define estimator and fit data\n",
    "    estimator = DecisionTreeClassifier()\n",
    "    pipeline = Pipeline(stages=[assembler,estimator])\n",
    "    tree_model = pipeline.fit(train)\n",
    "\n",
    "    # get results\n",
    "    results = tree_model.transform(test)\n",
    "    precision_recall(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Visualize the results of tree classifier ###\n",
    "pca_subset = pca_result.select('id','features','x_coord','y_coord')\n",
    "viz_df = pca_subset.join(results,on=['id','features']).select('x_coord','y_coord','prediction','label')\n",
    "viz_df = viz_df.dropna(how='any',subset=['x_coord','y_coord']).toPandas()\n",
    "\n",
    "a = sns.lmplot(x='x_coord',y='y_coord',hue='prediction',col='label',data=viz_df,\n",
    "           fit_reg=False,palette={1:'red',0:'green'},size=6,aspect=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Pipeline\n",
    "- K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######## Kmeans clustering, join to the PCA set #########\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(k=2,featuresCol='features')\n",
    "model = kmeans.fit(pca_result)\n",
    "centers = model.clusterCenters()\n",
    "kmeans_result = model.transform(pca_result).select('id','features','prediction')\n",
    "\n",
    "viz_df = pca_result.join(kmeans_result,on=['id','features'],how='inner')\\\n",
    "                   .select('id','features','x_coord','y_coord','prediction','label')\n",
    "\n",
    "viz_df = viz_df.dropna(how='any',subset=['x_coord','y_coord']).toPandas()\n",
    "\n",
    "sns.lmplot(x='x_coord',y='y_coord',hue='prediction',col='label',data=viz_df,\n",
    "           fit_reg=False,palette={1:'red',0:'green'},size=6,aspect=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Deploy to Cloud by using Watson Machine Learning Repo\n",
    "   Please follow demonstration! :D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}