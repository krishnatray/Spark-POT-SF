{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Spark SQL & Machine learning\n",
    "   \n",
    "   - RDD , Dataset APIs\n",
    "   - Dataset = DataFrame\n",
    "   - In this notebook, we will work through a dataset to demostrate spark's SQL-like abilities. We will also look at its machine learning capabilities in context of this dataset.\n",
    "    \n",
    "### Dataset\n",
    "- The chosen dataset is that of Breast Cancer. This set is collected from digitized image of a fine needle aspirate (FNA) of a breast mass. For more information about this dataset, please visit https://www.kaggle.com/uciml/breast-cancer-wisconsin-data\n",
    "- Each row contains information about a single breast mass along with a diagnosis of malignant/benign for this mass\n",
    "    \n",
    "### Problem statement\n",
    "- We need to learn patterns describing malignant and benign masses and need to be able to place any future samples in either of these buckets.  \n",
    "- Thus, the problem is a classification one where given a set of probable results, we need to pick one result with confidance. \n",
    "    \n",
    "### Solution\n",
    "   We will go through the following steps:\n",
    "   - Read data (Spark SQL)\n",
    "   - Feature Engineering (Accumulators, Broadcasters, ml/mllib APIs)\n",
    "   - Data Visualizations (PixieDust, Seaborn)\n",
    "   - Modeling (Spark ML)\n",
    "   - Evaluation and prediction (Spark ML)\n",
    "   - Deployment (Watson ML repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning process \n",
    "\n",
    "<center><img width=\"600px\" height=\"600px\" src=\"https://raw.githubusercontent.com/martinkearn/Content/master/Blogs/Images/MLProcess.PNG\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType, StringType, IntegerType, DoubleType, ArrayType\n",
    "\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "from pyspark import AccumulatorParam\n",
    "from pyspark import Broadcast\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from objectStore (can be any source, really)\n",
    " Data can be read in any format (as applicable to the native format), most common use cases are reading as json,csv, text or parquet files \n",
    " \n",
    " \n",
    " Click on the data file and ask to \"Insert as SparkSession Table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = df_data_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img width=\"450px\" height=\"450px\" margin=\"auto\" src=\"https://img.clipartfest.com/f92b25f421c985eed2ccb12cdb4cbf54_vector-clip-art-cartoon-safari-kids-cartoon-clipart_800-557.jpeg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic operations on dataframe \n",
    "- To view contents: df.show(), df.take(n)\n",
    "- To transform columns: df.withColumn (\"column_name\",\"transformation\")\n",
    "- To rename columns: df.withColumnRenamed(\"old_name\",\"new_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print first 3 rows of the input dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### check datatypes\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### make sure the data types are correct ####\n",
    "for col in data.columns:\n",
    "    if col not in ['id','diagnosis']:\n",
    "        data = data.withColumn(col,data[col].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace Null values with 0 ###\n",
    "Hint: This functionality is a part of the dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL\n",
    "\n",
    "#### Register dataframe as a temp table to query from (write sql on dataframes)\n",
    "\n",
    "- Hint: This functionality is a part of the dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying data\n",
    "- NOTE that the returned object is another Dataframe\n",
    "- One nice feature of the notebooks and python is that we can show it in a table via Pandas\n",
    "- Remember to perform an action to get your results (sql queries are also transformations :))\n",
    "\n",
    "\n",
    "##### Lets poke our data a little....\n",
    "- Template: spark.sql(\"sql_query\")\n",
    "- Select the top few columns of your dataset\n",
    "\n",
    "<center><img height=\"400px\" width=\"400px\" src=\"https://lh3.googleusercontent.com/-VS2gCcIbrWQ/VjMJaQfpSXI/AAAAAAAA0QI/ESvUyTWBb58/w1280-h720/1012286.gif\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple groupby example\n",
    "   \n",
    "   - Write a query to select the types of diagnosis and their counts\n",
    "    \n",
    "#### Hint: use groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "   - Dataset = Features + target\n",
    "   - Transforming features (categorical to numeric, continous to bins, scaling, normalization etc...)\n",
    "   - Selecting a subset of columns for wide datasets\n",
    "   - Exploding columns to make additional (synthetic features) for small datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature set is numeric\n",
    "\n",
    "##### Step 1: Make sure your target is numeric\n",
    "- \"target\" is categorical (malignant/benign)\n",
    "- Our encoding: 1 = malignant(M), 0 = Benign(B) using sql like \"when\" statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review your data\n",
    "#### Sample your data to view what it looks like before we make changes to the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make label numeric\n",
    "\n",
    "##### copy and paste to run code\n",
    "    data = data.withColumn('diagnosis',F.when(data.diagnosis=='M',1).otherwise(0))\n",
    "    data.toPandas().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename column from \"diagnosis\" to \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********************************************************************************\n",
    "\n",
    "###   Detour  \n",
    "- Feature engineering is a major task and often involves complicated scripts\n",
    "\n",
    "What happens when\n",
    "   - Have custom script based on other datasets/static variables?\n",
    "   - I want to port over a current script to spark ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulators, Broadcasters\n",
    "- One of the most basic things in scripts are variables (data and metadata). \n",
    "- Data vars = DataFrames/RDDs in spark\n",
    "- Metadata vars = Accumulators/broadcasters \n",
    "   \n",
    "- Metadata vars are not straightforward in the distributed world\n",
    "\n",
    "\n",
    "##### Accumulators\n",
    "   - Global variables which can be written into\n",
    "   \n",
    "##### Broadcasters\n",
    "   - Global variables to be read from \n",
    "************************************************************************************************\n",
    "\n",
    "\n",
    "#### Step 2: Select a subset of columns\n",
    "- Remove columns conveying very little information\n",
    "   - Zero variance columns\n",
    "   - Low correlation columns\n",
    "\n",
    "##### Zero variance test\n",
    "   - Let us write this test ourselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DictAccumulatorParam(AccumulatorParam):\n",
    "    def zero(self,initialValue):\n",
    "        return initialValue\n",
    "\n",
    "    def addInPlace(self, v1, v2):\n",
    "        v1.update(v2)\n",
    "        return v1\n",
    "    \n",
    "    \n",
    "def test_zero_variance(d,acc):\n",
    "    global df_pd\n",
    "    col = d[0]\n",
    "    value = d[1]\n",
    "    try:\n",
    "        mean = df_pd.value['mean'][col]\n",
    "        std = df_pd.value['stddev'][col]\n",
    "        if float(value)==float(mean) and float(std)==0.0:\n",
    "            acc.add({col:True})\n",
    "            return True\n",
    "        else:\n",
    "            acc.add({col:False})\n",
    "            return False\n",
    "    except KeyError:\n",
    "        acc.add({col:False})\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def count_labels(row):\n",
    "    global benign, malignant\n",
    "    if row.label==1:\n",
    "        malignant.add(1)\n",
    "    else:\n",
    "        benign.add(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcast the data variable so that each executor has its own version of it\n",
    "   - This way, it does need not be shipped to the executor with every call\n",
    "   - Saves network bandwidth\n",
    "\n",
    "##### copy and paste code from below\n",
    "    ### Create a broadcast variable with the summary stats ###\n",
    "    data = data.withColumn('dummy',F.lit(100))\n",
    "\n",
    "    ### Correct data types of variables prior to doing statistics ###\n",
    "    for col in data.columns:\n",
    "        if col not in ['id','diagnosis']:\n",
    "            data = data.withColumn(col,data[col].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect statistics for your dataset ###\n",
    "- Collect statistics for mean/std\n",
    "- store it in a variable called \"df\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert it to a pandas frame and broadcast it\n",
    "\n",
    "##### copy and run code\n",
    "    df_pd = df.toPandas()\n",
    "    df_pd.index = df_pd[\"summary\"]\n",
    "    del df_pd[\"summary\"]\n",
    "    df_pd = df_pd.transpose()\n",
    "\n",
    "    df_pd = sc.broadcast(df_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and accumulate\n",
    "\n",
    "##### copy and paste code\n",
    "\n",
    "    acc = sc.accumulator({}, DictAccumulatorParam())  \n",
    "    values = data.first().asDict()\n",
    "    input_value = sc.parallelize(values.items())\n",
    "    dummy = input_value.map(lambda x:test_zero_variance(x,acc))\\\n",
    "                      .map(lambda x:(x,1))\\\n",
    "                      .reduceByKey(lambda x,y:x+y)\n",
    "    dummy_collect = dummy.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### view the contents of accumulator\n",
    "    acc.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets run a simpler accumulator\n",
    "- Problem: Count the number of benign and malignant cases\n",
    "    \n",
    "#### copy and run this code\n",
    "    benign = sc.accumulator(0)\n",
    "    malignant = sc.accumulator(0)\n",
    "\n",
    "    data.rdd.foreach(count_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View contents of those accumulators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing zero variance columns ###\n",
    "\n",
    "##### copy and run code from below\n",
    "    for k,v in acc.value.items():\n",
    "        if v==True:\n",
    "            data = data.drop(k)\n",
    "\n",
    "    df = data.toPandas()\n",
    "    col_corr = sorted(df.corr()['label'].to_dict().items(),key=lambda x:x[1],reverse=True)\n",
    "    for col in col_corr[-5:]:\n",
    "        if col[0]!='id':\n",
    "            data = data.drop(col[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove columns with low correlation, such columns contribute to very little information\n",
    "- Caution: Be wary of multicolinearity (\"leaky\" columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets do some graphs!\n",
    "\n",
    "###  Here, we look at the columns and their correlations with the target variable.\n",
    "- We use PixieDust (charting library) for quick charts\n",
    "- Also takes RDDs as inputs, most of the other charting librabries take pandas dataframes as inputs\n",
    "\n",
    "\n",
    "##### copy and run code from here\n",
    "    import pixiedust\n",
    "    \n",
    "    df = pd.DataFrame(col_corr)\n",
    "    df = df.dropna()\n",
    "    df.columns = ['name','correlation']\n",
    "    viz_spark_df = sqlContext.createDataFrame(df)\n",
    "\n",
    "   \n",
    "    display(viz_spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pixiedust": {
     "displayParams": {
      "handlerId": "dataframe"
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize correlations ####\n",
    " - Useful for datasets with small number of columns\n",
    " - A heatmap is useful to visualize how variables are related to each other (and not just the target)\n",
    " - Very useful while doing NLP applications to visualize similarity between documents (after using TFIDF)\n",
    " \n",
    " \n",
    "##### copy and run code from below\n",
    "    plt.style.use('ggplot')\n",
    "    values = data.rdd.map(lambda x:list(x.asDict().values()))\n",
    "    corr_values = Statistics.corr(values)\n",
    "    names = data.rdd.map(lambda x:list(x.asDict().keys())).first()\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    sns.heatmap(corr_values,xticklabels=names,yticklabels=names,square=True,vmin=0, vmax=1,\n",
    "                    cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are all set to do some machine learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img width=\"350px\" height=\"350px\" margin=\"auto\" src=\"http://www.clipartkid.com/images/127/cartoon-explorer-characters-vectors-6cbSmI-clipart.jpg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Module layout\n",
    "##### It divides into two packages:\n",
    "    - spark.mllib contains the original API built on top of RDDs.\n",
    "    - spark.ml provides higher-level API built on top of DataFrames for constructing ML pipelines.\n",
    "    - Using spark.ml is recommended because with DataFrames the API is more versatile and flexible. But we will keep supporting spark.mllib along with the development of spark.ml. Users should be comfortable using spark.mllib features and expect more features coming.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A common receipe\n",
    "- Make sure your data is numeric \n",
    "- Collect feature columns\n",
    "- Define transformations (Vector Indexer, Encoder)\n",
    "- Assemble columns into a single Vector column (VectorAssembler)\n",
    "- Split data into test and train\n",
    "- Define pipelines and estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA  - Dimensionality Reduction \n",
    " - Very handy for visualization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###### PCA Visualize data ######\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "\n",
    "feature_cols = list(filter(lambda x:x not in ['id','label','dummy'],data.columns))\n",
    "assembler = VectorAssembler(inputCols=feature_cols,outputCol='features')\n",
    "df = assembler.transform(data)\n",
    "\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(df)\n",
    "\n",
    "pca_result = model.transform(df)\n",
    "result = pca_result.select(\"pcaFeatures\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UserDefined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "def g(point,co_ord):\n",
    "    if co_ord=='x':\n",
    "        value = regex.split('\\s+',point)[0][1:]\n",
    "    else:\n",
    "        value =  regex.split('\\s+',point)[1][:-1]\n",
    "    return value\n",
    "\n",
    "get_x_coord = UserDefinedFunction(lambda point:g(point,'x'), StringType())\n",
    "get_y_coord = UserDefinedFunction(lambda point:g(point,'y'), StringType())\n",
    "convert_to_string = UserDefinedFunction(lambda point:str(point.toArray()),StringType())\n",
    "\n",
    "pca_result = pca_result.withColumn('pcaFeatures_string',convert_to_string(pca_result['pcaFeatures']))\n",
    "pca_result = pca_result.withColumn('x_coord',get_x_coord(pca_result['pcaFeatures_string']))\\\n",
    "                       .withColumn('y_coord',get_y_coord(pca_result['pcaFeatures_string']))\\\n",
    "                      \n",
    "pca_result = pca_result.withColumn('x_coord',pca_result.x_coord.cast(FloatType()))\\\n",
    "                       .withColumn('y_coord',pca_result.y_coord.cast(FloatType()))\n",
    "    \n",
    "viz_df = pca_result.select('x_coord','y_coord','label').toPandas()\n",
    "a = sns.lmplot(x='x_coord',y='y_coord',hue='label',data=viz_df,\n",
    "           fit_reg=False,palette={1:'red',0:'green'},size=6,aspect=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset for machine learning\n",
    "    - Split data between test and train (70/30 split)\n",
    "    - Apply logistic regression and decision tree classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert 'label' to DoubleType()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split dataset into test and train sets (70/30) splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of records in test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "- Easy to use API\n",
    "- Define stages to transform your dataset\n",
    "- Select any estimator (model) for prediction \n",
    "- Each pipeline will model a single estimator onto your model\n",
    "\n",
    "\n",
    "<center><img src=\"https://image.slidesharecdn.com/mlwithapachespark-2-160818115257/95/introduction-to-ml-with-apache-spark-mllib-47-638.jpg?cb=1490306278\" width=\"500px\" height=\"500px\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Assembler to combine feature into a single vetor (feature vector! :) )\n",
    "\n",
    "#### copy and run code\n",
    "    from pyspark.ml.linalg import Vectors #linear algebra package, has matrices, arrays, Vectors (dense and sparse)\n",
    "    from pyspark.ml.feature import VectorAssembler \n",
    "\n",
    "    feature_cols = list(filter(lambda x:x not in ['id','label'],data.columns))\n",
    "    assembler = VectorAssembler(inputCols=feature_cols,outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Pipelines \n",
    "- Logistic Regression\n",
    "- Tree based classifier\n",
    "\n",
    "- Classification models work towards making a Yes/No decision\n",
    "- Metrics to evaluate the model: Precision/Recall, AUC\n",
    "\n",
    "\n",
    "<center><img width=\"500px\" height=\"500px\" src=\"http://opexanalytics.com/cnt/uploads/2016/01/Red-Fish-High-Recall.jpg\"></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Classification metrics ####\n",
    "def calc_metrics(results):\n",
    "    metrics = {}\n",
    "    metrics['tp_0'] = results.filter((results.label==0)&(results.prediction==0)).count()\n",
    "    metrics['fn_0'] = results.filter((results.label==0)&(results.prediction==1)).count()\n",
    "    metrics['tn_0'] = results.filter((results.label==1)&(results.prediction==1)).count()\n",
    "    metrics['fp_0'] = results.filter((results.label==1)&(results.prediction==0)).count()\n",
    "    \n",
    "    metrics['tp_1'] = results.filter((results.label==1)&(results.prediction==1)).count()\n",
    "    metrics['fn_1'] = results.filter((results.label==1)&(results.prediction==0)).count()\n",
    "    metrics['tn_1'] = results.filter((results.label==0)&(results.prediction==0)).count()\n",
    "    metrics['fp_1'] = results.filter((results.label==0)&(results.prediction==1)).count()\n",
    "    \n",
    "    return metrics\n",
    "        \n",
    "### calc precision & recall ###\n",
    "def precision_recall(results):\n",
    "    items = calc_metrics(results)\n",
    "    pre_0 = items['tp_0']/float((items['tp_0']+items['fp_0']))\n",
    "    pre_1 = items['tp_1']/float((items['tp_1']+items['fp_1']))\n",
    "    \n",
    "    recall_0 = items['tp_0']/float((items['tp_0']+items['fn_0']))\n",
    "    recall_1 = items['tp_1']/float((items['tp_1']+items['fn_1']))\n",
    "    \n",
    "    return {'pre_0':pre_0, 'recall_0':recall_0,'pre_1':pre_1,'recall_1':recall_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic regression #####\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "    #define estimator and fit data\n",
    "    estimator = LogisticRegression()\n",
    "    pipeline = Pipeline(stages=[assembler,estimator])\n",
    "    lr_model = pipeline.fit(train)\n",
    "\n",
    "    #get results\n",
    "    results = lr_model.transform(test)\n",
    "    precision_recall(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Visualize results from logistic regression ###\n",
    "pca_subset = pca_result.select('id','features','x_coord','y_coord')\n",
    "viz_df = pca_subset.join(results,on=['id','features']).select('x_coord','y_coord','prediction','label')\n",
    "viz_df = viz_df.dropna(how='any',subset=['x_coord','y_coord']).toPandas()\n",
    "\n",
    "a = sns.lmplot(x='x_coord',y='y_coord',hue='prediction',col='label',data=viz_df,\n",
    "           fit_reg=False,palette={1:'red',0:'green'},size=6,aspect=1)\n",
    "a.set_titles(col_template=['Decision Tree results when label==\"Benign\"','Decision Tree results when label==\"Malignant\"'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree classifier ####\n",
    "- Can you do the same using a decicion tree classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Visualize the results of tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Pipeline\n",
    "- K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######## Kmeans clustering, join to the PCA set #########\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(k=2,featuresCol='features')\n",
    "model = kmeans.fit(pca_result)\n",
    "centers = model.clusterCenters()\n",
    "kmeans_result = model.transform(pca_result).select('id','features','prediction')\n",
    "\n",
    "viz_df = pca_result.join(kmeans_result,on=['id','features'],how='inner')\\\n",
    "                   .select('id','features','x_coord','y_coord','prediction','label')\n",
    "\n",
    "viz_df = viz_df.dropna(how='any',subset=['x_coord','y_coord']).toPandas()\n",
    "\n",
    "sns.lmplot(x='x_coord',y='y_coord',hue='prediction',col='label',data=viz_df,\n",
    "           fit_reg=False,palette={1:'red',0:'green'},size=6,aspect=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Deploy to Cloud by using Watson Machine Learning Repo\n",
    "   Please follow demonstration! :D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
