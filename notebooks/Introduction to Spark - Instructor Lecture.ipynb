{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"http://www.cio-today.com/images/super/larger-15-IBM-Analytics-ApacheSpark1.jpg\" />\n",
    "\n",
    "<h1>Introduction to Spark</h1>\n",
    "<h3>\n",
    "This is a brief introduction to Spark, the purpose is to explain how Spark fits into the Data Analytics ecosystem and when, where and how to use it. Hope fully using some specific examples and common approaches you will better understand Spark after walking through this notebook</h3>\n",
    "<h3>Agenda:</h3>\n",
    "<ul>\n",
    "<li>What Spark IS</li>\n",
    "<li>What Spark is NOT</li>\n",
    "<li>When to use Spark</li>\n",
    "<li>Where does Spark fit into the ecosystem\\architecture</li>\n",
    "<li>What are the compontents of Spark</li>\n",
    "<li>Using Spark</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Apache Spark™ is a fast and general engine for large-scale data processing.</h2>\n",
    "\n",
    "As defined on the Apache Project page, 4 key value propisitions:\n",
    "<ul>\n",
    "<li>Speed</li>\n",
    "<li>Ease of Use</li>\n",
    "<li>Generality</li>\n",
    "<li>Runs Everywhere</li>\n",
    "</ul>\n",
    "\n",
    "Source: http://spark.apache.org/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>What Spark IS</h1>\n",
    "<ul>\n",
    "<li>Scaleable Processing Engine</li>\n",
    "<li>Polyglot</li>\n",
    "    <ul>\n",
    "        <li>Python, Scala, JAVA, SQL, R</li>\n",
    "    </ul>\n",
    "<li>Great alternative to most Map\\Reduce type computations</li>\n",
    "<li>Can handle streaming and batch mode, YMMV</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>&nbsp;</h3>\n",
    "<h3>&nbsp;</h3>\n",
    "<table style=\"height: 281px;\" border=\"1\" width=\"683\">\n",
    "<tbody>\n",
    "<tr>\n",
    "<td style=\"text-align: center;\"><strong>Spark</strong></td>\n",
    "<td style=\"text-align: center;\"><strong>Not Spark</strong></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&nbsp;Scale is known to be larger than a single computer\\server</td>\n",
    "<td>The dataset is small and will not grow&nbsp;</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&nbsp;Portability is key, for example running the same algorithm in batch and<span>&nbsp; </span>streaming</td>\n",
    "<td>&nbsp;Simple operations that can be run in the storage environment, for example SQL queries to a database</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Many different personas are going to use the environment: Analysts, Data Scientists, Data Engineers, Business Users&nbsp;</td>\n",
    "<td>&nbsp;Simple single use operations, for example storing the data in HDFS</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&nbsp;There is a need to leverage advanced data analysis, for example Machine Learning</td>\n",
    "<td>&nbsp;The type of analysis is already in an existing platform and known not to change</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Prototyping and designing data pipelines</td>\n",
    "<td>Engineering very specific, extremely high performance reads\\writes</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<h3>&nbsp;</h3>\n",
    "<h3>&nbsp;</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Where does Spark fit into an architecture?</h3></center>\n",
    "<table>\n",
    " <tr >\n",
    "  <td width=324 valign=top style='width:323.75pt;border:solid windowtext 1.0pt;\n",
    "  mso-border-alt:solid windowtext .5pt;padding:0in 5.4pt 0in 5.4pt'>\n",
    "  <p class=MsoNormal style='margin-left:.5in;text-indent:-.25in;mso-list:l0 level1 lfo2;\n",
    "  tab-stops:list .5in'><span style='font-family:Arial;\n",
    "  mso-fareast-font-family:Arial'><span style='mso-list:Ignore'>•<span\n",
    "  style='font:7.0pt \"Times New Roman\"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span>Above\n",
    "  the storage layer</p>\n",
    "  <p class=MsoNormal style='margin-left:.5in;text-indent:-.25in;mso-list:l0 level1 lfo2;\n",
    "  tab-stops:list .5in'><span style='font-family:Arial;\n",
    "  mso-fareast-font-family:Arial'><span style='mso-list:Ignore'>•<span\n",
    "  style='font:7.0pt \"Times New Roman\"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span>Can\n",
    "  still leverage HDFS</p>\n",
    "  <p class=MsoNormal style='margin-left:.5in;text-indent:-.25in;mso-list:l0 level1 lfo2;\n",
    "  tab-stops:list .5in'><span style='font-family:Arial;\n",
    "  mso-fareast-font-family:Arial'><span style='mso-list:Ignore'>•<span\n",
    "  style='font:7.0pt \"Times New Roman\"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span>Can\n",
    "  also connect to DBs</p>\n",
    "  <p class=MsoNormal style='margin-left:.5in;text-indent:-.25in;mso-list:l0 level1 lfo2;\n",
    "  tab-stops:list .5in'><span style='font-family:Arial;\n",
    "  mso-fareast-font-family:Arial'><span style='mso-list:Ignore'>•<span\n",
    "  style='font:7.0pt \"Times New Roman\"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span>Much\n",
    "  more efficient because of:</p>\n",
    "  <p class=MsoNormal style='margin-left:1.0in;text-indent:-.25in;mso-list:l0 level2 lfo2;\n",
    "  tab-stops:list 1.0in'><span style='font-family:Arial;\n",
    "  mso-fareast-font-family:Arial'><span style='mso-list:Ignore'>•<span\n",
    "  style='font:7.0pt \"Times New Roman\"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span>Memory\n",
    "  Utilization</p>\n",
    "  <p class=MsoNormal style='margin-left:1.0in;text-indent:-.25in;mso-list:l0 level2 lfo2;\n",
    "  tab-stops:list 1.0in'><span style='font-family:Arial;\n",
    "  mso-fareast-font-family:Arial'><span style='mso-list:Ignore'>•<span\n",
    "  style='font:7.0pt \"Times New Roman\"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span>Simplicity\n",
    "  of Development</p>\n",
    "  <p class=MsoNormal style='margin-left:1.0in;text-indent:-.25in;mso-list:l0 level2 lfo2;\n",
    "  tab-stops:list 1.0in'><span style='font-family:Arial;\n",
    "  mso-fareast-font-family:Arial'><span style='mso-list:Ignore'>•<span\n",
    "  style='font:7.0pt \"Times New Roman\"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span>Broader\n",
    "  Audience (Python, Scala)</p>\n",
    "  </td>\n",
    "  <td>\n",
    "  <img src=\"https://raw.githubusercontent.com/bradenrc/Spark_POT/master/Modules/SparkIntro/images/hadoop.png\" width=\"90%\" height=\"90%\"  />\n",
    "  </td>\n",
    " </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h2>Another Perspective</h2>\n",
    "</center>\n",
    "<table>\n",
    " <tr >\n",
    "  <td>\n",
    "  <h2>Includes:</h2>\n",
    "<ul>\n",
    "    <li>Core Engine</li>\n",
    "    <li>Modules for specific types of operations:</li>\n",
    "    <ul>\n",
    "    <li>GraphX</li>\n",
    "    <li>R</li>\n",
    "    <li>SQL</li>\n",
    "    <li>Machine Learning</li>\n",
    "    <li>Streaming</li>\n",
    "    </ul>\n",
    "</ul>\n",
    "  </td>\n",
    "  <td >\n",
    "  <img src=\"https://raw.githubusercontent.com/bradenrc/Spark_POT/master/Modules/SparkIntro/images/spark_arch.png\" width=\"90%\" height=\"90%\" />\n",
    "  </td>\n",
    " </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Using Spark</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Spark Driver and Workers programs</h2>\n",
    "A Spark program has a driver program and a workers program. Worker programs run on cluster nodes or in local threads. RDDs are distributed\u001d",
    " across workers. \n",
    "<img src='https://github.com/carloapp2/SparkPOT/blob/master/Spark%20Architecture.png?raw=true' width=\"50%\" height=\"50%\"></img>\n",
    "\n",
    "<h2>Resilient Distributed Dataset (RDD)</h2>\n",
    "The basic abstration of data storage.\n",
    "<img src=\"https://dzone.com/storage/rc-covers/6617-thumb.png\" width=\"60%\" height=\"60%\" />\n",
    "\n",
    "Keep in mind one can construct RDDs a number of ways that include \n",
    "-  Parallelizing existing Python collections (lists) \n",
    "-  Transforming an existing RDDs  \n",
    "-  From files in HDFS or any other storage system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 10]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "#An Example:\n",
    "\n",
    "#Step 1 We can create a simple Python List and Turn it into a Spark RDD\n",
    "\n",
    "python_list = [1, 2, 3, 4, 5, 6, 7, 8, 10]\n",
    "\n",
    "print python_list\n",
    "print len(python_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Step 2 use the sc libarary, note in our notebook enviroment this is already instantiated\n",
    "first_rdd = sc.parallelize(python_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:\n",
      "9\n",
      "****************************************\n",
      "First Item:\n",
      "1\n",
      "****************************************\n",
      "Max Value:\n",
      "10\n",
      "****************************************\n",
      "Sum Value:\n",
      "46\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "#Step 3 Explore the RDD\n",
    "print \"Count:\"\n",
    "print first_rdd.count()\n",
    "print \"*\" * 40\n",
    "\n",
    "print \"First Item:\"\n",
    "print first_rdd.first()\n",
    "print \"*\" * 40\n",
    "\n",
    "print \"Max Value:\"\n",
    "print first_rdd.max()\n",
    "print \"*\" * 40\n",
    "\n",
    "print \"Sum Value:\"\n",
    "print first_rdd.sum()\n",
    "print \"*\" * 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "****************************************\n",
      "Iterate first 5 Values:\n",
      "[1, 1]\n",
      "[2, 4]\n",
      "[3, 9]\n",
      "[4, 16]\n",
      "[5, 25]\n",
      "****************************************\n",
      "Also note the type returned by take is a Python List\n",
      "<type 'list'>\n",
      "****************************************\n",
      "Reduce Operation, returning the final value:\n",
      "46\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "#Step 4 - You can perform map and reduce operations as well. Note the returned object type is another RDD\n",
    "\n",
    "#Map Operation creating a new RDD\n",
    "second_rdd = first_rdd.map(lambda x: [x, x**2])\n",
    "\n",
    "print \"Type:\"\n",
    "print type(second_rdd)\n",
    "print \"*\" * 40\n",
    "\n",
    "\n",
    "print \"Iterate first 5 Values:\"\n",
    "\n",
    "for value in second_rdd.take(5):\n",
    "    print value\n",
    "\n",
    "print \"*\" * 40\n",
    "\n",
    "print \"Also note the type returned by take is a Python List\"\n",
    "print type(second_rdd.take(5))\n",
    "print \"*\" * 40\n",
    "\n",
    "print \"Reduce Operation, returning the final value:\"\n",
    "print first_rdd.reduce(lambda accum, n: accum + n)\n",
    "print \"*\" * 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's put it into the common word count example:\n",
    "large_string = \"\"\"\n",
    "Community\n",
    "Spark is used at a wide range of organizations to process large datasets. You can find example use cases at the Spark Summit conference, or on the Powered By page.\n",
    "There are many ways to reach the community:\n",
    "    Use the mailing lists to ask questions.\n",
    "    In-person events include numerous meetup groups and Spark Summit.\n",
    "    We use JIRA for issue tracking.\n",
    "\n",
    "Contributors\n",
    "Apache Spark is built by a wide set of developers from over 200 companies. Since 2009, more than 1000 developers have contributed to Spark!\n",
    "The project's committers come from 19 organizations.\n",
    "If you'd like to participate in Spark, or contribute to the libraries on top of it, learn how to contribute.\n",
    "\n",
    "Getting Started\n",
    "Learning Spark is easy whether you come from a Java or Python background:\n",
    "    Download the latest release — you can run Spark locally on your laptop.\n",
    "    Read the quick start guide.\n",
    "    Spark Summit 2014 contained free training videos and exercises.\n",
    "    Learn how to deploy Spark on a cluster.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195\n",
      "\n",
      "Community\n",
      "Spark\n",
      "is\n",
      "used\n",
      "at\n",
      "a\n",
      "wide\n",
      "range\n",
      "of\n"
     ]
    }
   ],
   "source": [
    "#Step 1 Build an RDD with the values:\n",
    "\n",
    "#Create a list of the string values (replace carriage returns):\n",
    "large_string_list = large_string.replace(\"\\n\", \" \").split(\" \")\n",
    "\n",
    "#Create the RDD\n",
    "large_string_rdd = sc.parallelize(large_string_list)\n",
    "\n",
    "#How Many Words are in the RDD:\n",
    "print large_string_rdd.count()\n",
    "\n",
    "#Show the first 10 Values:\n",
    "for value in large_string_rdd.take(10):\n",
    "    print value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['', 1], ['Community', 1], ['Spark', 1], ['is', 1], ['used', 1]]\n"
     ]
    }
   ],
   "source": [
    "#Step 2 add a single value to the end of each word that can be summed up\n",
    "#for example [[Community, 1], [Spark, 1], [is, 1], etc...]\n",
    "\n",
    "words_rdd = large_string_rdd.map(lambda x: [x, 1])\n",
    "print words_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', 32)\n",
      "('and', 2)\n",
      "('Use', 1)\n",
      "('We', 1)\n",
      "('Java', 1)\n",
      "('Summit.', 1)\n",
      "('videos', 1)\n",
      "('process', 1)\n",
      "('is', 3)\n",
      "('JIRA', 1)\n",
      "('Contributors', 1)\n",
      "('2009,', 1)\n",
      "('have', 1)\n",
      "('You', 1)\n",
      "('ways', 1)\n",
      "('Spark!', 1)\n",
      "('In-person', 1)\n",
      "('meetup', 1)\n",
      "('use', 2)\n",
      "('from', 3)\n"
     ]
    }
   ],
   "source": [
    "#Step 3 - Reduce By Key (Word), which returns an RDD\n",
    "word_count_rdd = words_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "for value in word_count_rdd.take(20):\n",
    "    print value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter:\n",
      "[('Spark!', 1), ('Spark', 8), ('Spark,', 1)]\n",
      "****************************************\n",
      "Sort:\n",
      "[('', 32), ('Spark', 8), ('to', 8), ('the', 7), ('a', 4)]\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "#You can run many different operations to explore the data, here are a couple of examples:\n",
    "\n",
    "#Filter for \"spark\"\n",
    "print \"Filter:\"\n",
    "print word_count_rdd.filter(lambda x: \"spark\" in x[0].lower()).collect()\n",
    "print \"*\" * 40\n",
    "\n",
    "#Sort (note inverse)\"\n",
    "print \"Sort:\"\n",
    "print word_count_rdd.sortBy(lambda x: x[1] * -1).take(5)\n",
    "print \"*\" * 40\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "large_combined_string_rdd Count: \n",
      "1\n",
      "****************************************\n",
      "large_combined_string_rdd First 5: \n",
      "[\"\\nCommunity\\nSpark is used at a wide range of organizations to process large datasets. You can find example use cases at the Spark Summit conference, or on the Powered By page.\\nThere are many ways to reach the community:\\n    Use the mailing lists to ask questions.\\n    In-person events include numerous meetup groups and Spark Summit.\\n    We use JIRA for issue tracking.\\n\\nContributors\\nApache Spark is built by a wide set of developers from over 200 companies. Since 2009, more than 1000 developers have contributed to Spark!\\nThe project's committers come from 19 organizations.\\nIf you'd like to participate in Spark, or contribute to the libraries on top of it, learn how to contribute.\\n\\nGetting Started\\nLearning Spark is easy whether you come from a Java or Python background:\\n    Download the latest release \\xe2\\x80\\x94 you can run Spark locally on your laptop.\\n    Read the quick start guide.\\n    Spark Summit 2014 contained free training videos and exercises.\\n    Learn how to deploy Spark on a cluster.\\n\"]\n",
      "****************************************\n",
      "lcs_fm_rd Count: \n",
      "120\n",
      "****************************************\n",
      "lcs_fm_rd First 5: \n",
      "[('', 32), ('JIRA', 1), ('Use', 1), ('Read', 1), ('Java', 1)]\n",
      "****************************************\n",
      "lcs_fm_rd Top 5: \n",
      "[('', 32), ('Spark', 8), ('to', 8), ('the', 7), ('a', 4)]\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "#A Slightly better approach to the same problem\n",
    "#use Spark to split the string into a \"FlatMap\"\n",
    "\n",
    "#This is better since we use Spark to split and replace the values, not Python\n",
    "\n",
    "#Parallelize the entire string\n",
    "large_combined_string_rdd = sc.parallelize([large_string]) #must pass a single \"tuple\" as a list\n",
    "print \"large_combined_string_rdd Count: \"\n",
    "print large_combined_string_rdd.count()\n",
    "print \"*\" * 40 \n",
    "\n",
    "print \"large_combined_string_rdd First 5: \"\n",
    "print large_combined_string_rdd.take(5)\n",
    "print \"*\" * 40 \n",
    "\n",
    "\n",
    "#flatmap, map and reduceByKey\n",
    "lcs_fm_rd = large_combined_string_rdd.flatMap(lambda x: x.replace(\"\\n\", \" \").split(\" \")).\\\n",
    "                                              map(lambda x: [x, 1]).\\\n",
    "                                              reduceByKey(lambda a, b: a+b)\n",
    "\n",
    "print \"lcs_fm_rd Count: \"\n",
    "print lcs_fm_rd.count()\n",
    "print \"*\" * 40 \n",
    "\n",
    "print \"lcs_fm_rd First 5: \"\n",
    "print lcs_fm_rd.take(5)\n",
    "print \"*\" * 40 \n",
    "\n",
    "print \"lcs_fm_rd Top 5: \"\n",
    "print lcs_fm_rd.sortBy(lambda x: x[1] * -1).take(5)\n",
    "print \"*\" * 40 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Lazy Evaluation</h2>\n",
    "A key part of the performance of Spark is lazy evaluation.\n",
    "<br>\n",
    "Recall this diagram:\n",
    "<img src=\"https://dzone.com/storage/rc-covers/6617-thumb.png\" width=\"50%\" height=\"50%\" />\n",
    "\n",
    "Note that the <b>Transformations</b> are not executed until we call an <b>Action</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 10]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A simple Example:\n",
    "\n",
    "#Our First RDD:\n",
    "first_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transformation, this is NOT evaluated, it is stored a possible operation\n",
    "second_rdd = first_rdd.map(lambda x: x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304\n"
     ]
    }
   ],
   "source": [
    "#Action, this IS evaluated\n",
    "print second_rdd.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"transformations\">Transformations</h3>\n",
    "\n",
    "<p>The following table lists some of the common transformations supported by Spark. Refer to the\n",
    "RDD API doc\n",
    "(<a href=\"api/scala/index.html#org.apache.spark.rdd.RDD\">Scala</a>,\n",
    " <a href=\"api/java/index.html?org/apache/spark/api/java/JavaRDD.html\">Java</a>,\n",
    " <a href=\"api/python/pyspark.html#pyspark.RDD\">Python</a>,\n",
    " <a href=\"api/R/index.html\">R</a>)\n",
    "and pair RDD functions doc\n",
    "(<a href=\"api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions\">Scala</a>,\n",
    " <a href=\"api/java/index.html?org/apache/spark/api/java/JavaPairRDD.html\">Java</a>)\n",
    "for details.</p>\n",
    "\n",
    "<table class=\"table\">\n",
    "<tr><th style=\"width:25%\">Transformation</th><th>Meaning</th></tr>\n",
    "<tr>\n",
    "  <td> <b>map</b>(<i>func</i>) </td>\n",
    "  <td> Return a new distributed dataset formed by passing each element of the source through a function <i>func</i>. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>filter</b>(<i>func</i>) </td>\n",
    "  <td> Return a new dataset formed by selecting those elements of the source on which <i>func</i> returns true. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>flatMap</b>(<i>func</i>) </td>\n",
    "  <td> Similar to map, but each input item can be mapped to 0 or more output items (so <i>func</i> should return a Seq rather than a single item). </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>mapPartitions</b>(<i>func</i>) <a name=\"MapPartLink\"></a> </td>\n",
    "  <td> Similar to map, but runs separately on each partition (block) of the RDD, so <i>func</i> must be of type\n",
    "    Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt; when running on an RDD of type T. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>mapPartitionsWithIndex</b>(<i>func</i>) </td>\n",
    "  <td> Similar to mapPartitions, but also provides <i>func</i> with an integer value representing the index of\n",
    "  the partition, so <i>func</i> must be of type (Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt; when running on an RDD of type T.\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>sample</b>(<i>withReplacement</i>, <i>fraction</i>, <i>seed</i>) </td>\n",
    "  <td> Sample a fraction <i>fraction</i> of the data, with or without replacement, using a given random number generator seed. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>union</b>(<i>otherDataset</i>) </td>\n",
    "  <td> Return a new dataset that contains the union of the elements in the source dataset and the argument. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>intersection</b>(<i>otherDataset</i>) </td>\n",
    "  <td> Return a new RDD that contains the intersection of elements in the source dataset and the argument. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>distinct</b>([<i>numTasks</i>])) </td>\n",
    "  <td> Return a new dataset that contains the distinct elements of the source dataset.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>groupByKey</b>([<i>numTasks</i>]) <a name=\"GroupByLink\"></a> </td>\n",
    "  <td> When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable&lt;V&gt;) pairs. <br />\n",
    "    <b>Note:</b> If you are grouping in order to perform an aggregation (such as a sum or\n",
    "      average) over each key, using <code>reduceByKey</code> or <code>aggregateByKey</code> will yield much better\n",
    "      performance.\n",
    "    <br />\n",
    "    <b>Note:</b> By default, the level of parallelism in the output depends on the number of partitions of the parent RDD.\n",
    "      You can pass an optional <code>numTasks</code> argument to set a different number of tasks.\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>reduceByKey</b>(<i>func</i>, [<i>numTasks</i>]) <a name=\"ReduceByLink\"></a> </td>\n",
    "  <td> When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function <i>func</i>, which must be of type (V,V) =&gt; V. Like in <code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>aggregateByKey</b>(<i>zeroValue</i>)(<i>seqOp</i>, <i>combOp</i>, [<i>numTasks</i>]) <a name=\"AggregateByLink\"></a> </td>\n",
    "  <td> When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral \"zero\" value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in <code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>sortByKey</b>([<i>ascending</i>], [<i>numTasks</i>]) <a name=\"SortByLink\"></a> </td>\n",
    "  <td> When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean <code>ascending</code> argument.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>join</b>(<i>otherDataset</i>, [<i>numTasks</i>]) <a name=\"JoinLink\"></a> </td>\n",
    "  <td> When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key.\n",
    "    Outer joins are supported through <code>leftOuterJoin</code>, <code>rightOuterJoin</code>, and <code>fullOuterJoin</code>.\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>cogroup</b>(<i>otherDataset</i>, [<i>numTasks</i>]) <a name=\"CogroupLink\"></a> </td>\n",
    "  <td> When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable&lt;V&gt;, Iterable&lt;W&gt;)) tuples. This operation is also called <code>groupWith</code>. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>cartesian</b>(<i>otherDataset</i>) </td>\n",
    "  <td> When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements). </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>pipe</b>(<i>command</i>, <i>[envVars]</i>) </td>\n",
    "  <td> Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the\n",
    "    process's stdin and lines output to its stdout are returned as an RDD of strings. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>coalesce</b>(<i>numPartitions</i>) <a name=\"CoalesceLink\"></a> </td>\n",
    "  <td> Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently\n",
    "    after filtering down a large dataset. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>repartition</b>(<i>numPartitions</i>) </td>\n",
    "  <td> Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them.\n",
    "    This always shuffles all data over the network. <a name=\"RepartitionLink\"></a></td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>repartitionAndSortWithinPartitions</b>(<i>partitioner</i>) <a name=\"Repartition2Link\"></a></td>\n",
    "  <td> Repartition the RDD according to the given partitioner and, within each resulting partition,\n",
    "  sort records by their keys. This is more efficient than calling <code>repartition</code> and then sorting within\n",
    "  each partition because it can push the sorting down into the shuffle machinery. </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"actions\">Actions</h3>\n",
    "\n",
    "<p>The following table lists some of the common actions supported by Spark. Refer to the\n",
    "RDD API doc\n",
    "(<a href=\"api/scala/index.html#org.apache.spark.rdd.RDD\">Scala</a>,\n",
    " <a href=\"api/java/index.html?org/apache/spark/api/java/JavaRDD.html\">Java</a>,\n",
    " <a href=\"api/python/pyspark.html#pyspark.RDD\">Python</a>,\n",
    " <a href=\"api/R/index.html\">R</a>)</p>\n",
    "\n",
    "<p>and pair RDD functions doc\n",
    "(<a href=\"api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions\">Scala</a>,\n",
    " <a href=\"api/java/index.html?org/apache/spark/api/java/JavaPairRDD.html\">Java</a>)\n",
    "for details.</p>\n",
    "\n",
    "<table class=\"table\">\n",
    "<tr><th>Action</th><th>Meaning</th></tr>\n",
    "<tr>\n",
    "  <td> <b>reduce</b>(<i>func</i>) </td>\n",
    "  <td> Aggregate the elements of the dataset using a function <i>func</i> (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>collect</b>() </td>\n",
    "  <td> Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>count</b>() </td>\n",
    "  <td> Return the number of elements in the dataset. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>first</b>() </td>\n",
    "  <td> Return the first element of the dataset (similar to take(1)). </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>take</b>(<i>n</i>) </td>\n",
    "  <td> Return an array with the first <i>n</i> elements of the dataset. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>takeSample</b>(<i>withReplacement</i>, <i>num</i>, [<i>seed</i>]) </td>\n",
    "  <td> Return an array with a random sample of <i>num</i> elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>takeOrdered</b>(<i>n</i>, <i>[ordering]</i>) </td>\n",
    "  <td> Return the first <i>n</i> elements of the RDD using either their natural order or a custom comparator. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>saveAsTextFile</b>(<i>path</i>) </td>\n",
    "  <td> Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>saveAsSequenceFile</b>(<i>path</i>) <br /> (Java and Scala) </td>\n",
    "  <td> Write the elements of the dataset as a Hadoop SequenceFile in a given path in the local filesystem, HDFS or any other Hadoop-supported file system. This is available on RDDs of key-value pairs that implement Hadoop's Writable interface. In Scala, it is also\n",
    "   available on types that are implicitly convertible to Writable (Spark includes conversions for basic types like Int, Double, String, etc). </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>saveAsObjectFile</b>(<i>path</i>) <br /> (Java and Scala) </td>\n",
    "  <td> Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using\n",
    "    <code>SparkContext.objectFile()</code>. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>countByKey</b>() <a name=\"CountByLink\"></a> </td>\n",
    "  <td> Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>foreach</b>(<i>func</i>) </td>\n",
    "  <td> Run a function <i>func</i> on each element of the dataset. This is usually done for side effects such as updating an <a href=\"#accumulators\">Accumulator</a> or interacting with external storage systems.\n",
    "  <br /><b>Note</b>: modifying variables other than Accumulators outside of the <code>foreach()</code> may result in undefined behavior. See <a href=\"#understanding-closures-a-nameclosureslinka\">Understanding closures </a> for more details.</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}